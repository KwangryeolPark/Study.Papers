# Study.Papers

ì—°êµ¬ ë…¼ë¬¸ì„ ì£¼ì œë³„ë¡œ ì •ë¦¬í•œ ì €ì¥ì†Œì…ë‹ˆë‹¤.

## ğŸ“š Time Series

### ğŸ”„ Multimodal

<details>
<summary><b>How Can Time Series Analysis Benefit From Multiple Modalities? A Survey and Outlook</b></summary>

**Links:** [PDF](papers/Time%20Series/Multimodal/How%20Can%20Time%20Series%20Analysis%20Benefit%20From%20Multiple%20Modalities%20A%20Survey%20and%20Outlook.pdf) | [Notes](notes/Time%20Series/Multimodal/How%20Can%20Time%20Series%20Analysis%20Benefit%20From%20Multiple%20Modalities%20A%20Survey%20and%20Outlook.md)

**ì„¤ëª…:** ì‹œê³„ì—´ ë¶„ì„ì´ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ì™€ ê²°í•©í•˜ì—¬ ë°œì „í•˜ëŠ” Multiple Modalities For TSA ë¶„ì•¼ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•œ ìµœì´ˆì˜ ì„œë² ì´ ë…¼ë¬¸. TimeAsX (íƒ€ ëª¨ë‹¬ë¦¬í‹° íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ ì¬ì‚¬ìš©), Time+X (ë©€í‹°ëª¨ë‹¬ í™•ì¥), Time2X & X2Time (êµì°¨ ëª¨ë‹¬ë¦¬í‹° ìƒí˜¸ì‘ìš©) ì„¸ ê°€ì§€ í•µì‹¬ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•˜ë©°, í…ìŠ¤íŠ¸ì˜ ì—­í• (Static/Dynamic Text)ê³¼ ë„ë©”ì¸ë³„ íŠ¹ì„±, ê·¸ë¦¬ê³  ì´ì§ˆì  ëª¨ë‹¬ë¦¬í‹° ì¡°í•© ì²˜ë¦¬ì˜ í•œê³„ë¥¼ ë‹¤ë£¬ë‹¤.

**Tags:** `#timeseries` `#multimodal` `#survey`

</details>

<details>
<summary><b>TIMER-XL: LONG-CONTEXT TRANSFORMERS FOR UNIFIED TIME SERIES FORECASTING</b></summary>

**Links:** [PDF]((Timer-XL)%20TIMER-XL%20LONG-CONTEXT%20TRANSFORMERS%20FOR%20UNIFIED%20TIME%20SERIES%20FORECASTING.pdf) | [Notes]((Timer-XL)%20TIMER-XL%20LONG-CONTEXT%20TRANSFORMERS%20FOR%20UNIFIED%20TIME%20SERIES%20FORECASTING.md)

**ì„¤ëª…:** Decoder-only Transformer ê¸°ë°˜ì˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë¡œ, ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ë³€ìˆ˜ ê°„ ì˜ì¡´ì„±ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ Time Attention ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆ. ì‚¬ìš©ì ì •ì˜ ê°€ëŠ¥í•œ ë³€ìˆ˜ ì˜ì¡´ì„± í–‰ë ¬(C matrix)ì„ í™œìš©í•˜ì—¬ covariateì™€ target ê°„ì˜ ê´€ê³„ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ë©°, ì‹œê³„ì—´ì—ì„œ ì¶©ë¶„íˆ ì—°êµ¬ë˜ì§€ ì•Šì€ position embedding ë¬¸ì œë¥¼ ë‹¤ë£¬ë‹¤.

**Tags:** `#timeseries` `#multimodal` `#transformer`

</details>

<details>
<summary><b>Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting</b></summary>

**Links:** [PDF]((Time-VLM)%20Time-VLM%20Exploring%20Multimodal%20Vision-Language%20Models%20for%20Augmented%20Time%20Series%20Forecasting.pdf) | [Notes]((Time-VLM)%20Time-VLM%20Exploring%20Multimodal%20Vision-Language%20Models%20for%20Augmented%20Time%20Series%20Forecasting.md)

**ì„¤ëª…:** Vision-Language Modelì„ ì‹œê³„ì—´ ì˜ˆì¸¡ì— í™œìš©í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ í”„ë ˆì„ì›Œí¬. Retrieval-Augmented Learner(ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬), Vision-Augmented Learner(ì‹œê³„ì—´ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜), Text-Augmented Learner(í†µê³„ì  íŠ¹ì„±ê³¼ ë„ë©”ì¸ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ìƒì„±)ì˜ ì„¸ ê°€ì§€ ëª¨ë“ˆì´ í˜‘ë ¥í•˜ì—¬ ì‹œê°„ì , ì‹œê°ì , í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í†µí•©í•œë‹¤. TextëŠ” ë¬¸ë§¥ ì´í•´ì—, Visionì€ ì„¸ë°€í•œ temporal pattern í¬ì°©ì— ê°•ì ì„ ê°€ì§„ë‹¤.

**Tags:** `#timeseries` `#multimodal` `#vision-language-model`

</details>

<details>
<summary><b>TimeXL: Explainable Multi-modal Time Series Prediction with LLM-in-the-Loop</b></summary>

**Links:** [PDF]((TimeXL)%20TimeXL%20Explainable%20Multi-modal%20Time%20Series%20Prediction%20with%20LLM-in-the-Loop.pdf) | [Notes]((TimeXL)%20TimeXL%20Explainable%20Multi-modal%20Time%20Series%20Prediction%20with%20LLM-in-the-Loop.md)

**ì„¤ëª…:** Prototype ê¸°ë°˜ encoderì™€ LLMì„ í™œìš©í•˜ì—¬ í•´ì„ ê°€ëŠ¥í•œ ë©€í‹°ëª¨ë‹¬ ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ì œê³µí•˜ëŠ” ëª¨ë¸. Case-based rationalesë¥¼ í†µí•´ ì˜ˆì¸¡ì— ëŒ€í•œ ì„¤ëª… ê°€ëŠ¥ì„±ì„ ì œê³µí•˜ë©°, ê¸ˆìœµì´ë‚˜ ì˜ë£Œ ê°™ì€ high-stakes ë¶„ì•¼ì—ì„œ ì¤‘ìš”í•œ í•´ì„ ê°€ëŠ¥ì„±ì„ ê°•ì¡°í•œë‹¤. ICML 2025ì—ì„œ rejectë˜ì—ˆìœ¼ë©°, ì£¼ëœ í”¼ë“œë°±ì€ ê° ì»´í¬ë„ŒíŠ¸ì˜ ì„±ëŠ¥ ê¸°ì—¬ë„ ë¶„ì„ ë¶€ì¡±ê³¼ LLM ì‚¬ìš© ëŒ€ë¹„ ë¯¸ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒ(0.9%p)ì´ì—ˆë‹¤.

**Tags:** `#timeseries` `#multimodal` `#explainability` `#llm`

</details>

<details>
<summary><b>XForecast: Evaluating Natural Language Explanations for Time Series Forecasting</b></summary>

**Links:** [PDF]((XForecast)%20XForecast%20Evaluating%20Natural%20Language%20Explanations%20for%20Time%20Series%20Forecasting.pdf) | [Notes]((XForecast)%20XForecast%20Evaluating%20Natural%20Language%20Explanations%20for%20Time%20Series%20Forecasting.md)

**ì„¤ëª…:** ì‹œê³„ì—´ ì˜ˆì¸¡ì—ì„œ ìì—°ì–´ ì„¤ëª…(Natural Language Explanation)ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ë¡ ì„ ë‹¤ë£¬ ë…¼ë¬¸. Saliency mapsì™€ ê°™ì€ ì‹œê°ì  ì¤‘ìš”ë„ í‘œí˜„ì„ ë„˜ì–´ ìì—°ì–´ë¡œ ì˜ˆì¸¡ ê·¼ê±°ë¥¼ ì œê³µí•˜ëŠ” ë°©ì‹ì„ íƒêµ¬í•œë‹¤.

**Tags:** `#timeseries` `#multimodal` `#explainability`

*(ì½ì§€ ì•ŠìŒ)*

</details>

### ğŸ¯ Representation Learning

<details>
<summary><b>(TimeSiam) TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling</b></summary>

**Links:** [PDF](papers/Time%20Series/Representation%20Learning/(TimeSiam)%20TimeSiam%20A%20Pre-Training%20Framework%20for%20Siamese%20Time-Series%20Modeling.pdf) | [Notes](notes/Time%20Series/Representation%20Learning/(TimeSiam)%20TimeSiam%20A%20Pre-Training%20Framework%20for%20Siamese%20Time-Series%20Modeling.md)

**ì„¤ëª…:** Siamese êµ¬ì¡°ë¥¼ í™œìš©í•œ ì‹œê³„ì—´ ë°ì´í„°ì˜ ì‚¬ì „ í•™ìŠµ í”„ë ˆì„ì›Œí¬. Model-agnosticí•œ ì ‘ê·¼ ë°©ì‹ì„ ì œì‹œí•œë‹¤.

**Tags:** `#timeseries` `#representation-learning` `#model-agnostic`

*(ì½ì§€ ì•ŠìŒ)*

</details>

<details>
<summary><b>(SoftCLT) SOFT CONTRASTIVE LEARNING FOR TIME SERIES</b></summary>

**Links:** [PDF](papers/Time%20Series/Representation%20Learning/(SoftCLT)%20SOFT%20CONTRASTIVE%20LEARNING%20FOR%20TIME%20SERIES.pdf) | [Notes](notes/Time%20Series/Representation%20Learning/(SoftCLT)%20SOFT%20CONTRASTIVE%20LEARNING%20FOR%20TIME%20SERIES.md)

**ì„¤ëª…:** Contrastive Learningì˜ Hard Negative ë°©ì‹ì„ ê°œì„ í•œ ë…¼ë¬¸. Negative pairë¥¼ ë¬´ì¡°ê±´ ë©€ê²Œ í•˜ëŠ” ëŒ€ì‹  ì ì ˆí•œ ê±°ë¦¬ë¡œ ë¶„ë¦¬í•˜ëŠ” Soft Contrastive Learningì„ ì œì•ˆí•œë‹¤. Instance-wise lossëŠ” DTW(Dynamic Time Warping) ê±°ë¦¬ì˜ min-max normalized ê°’ì„, Temporal lossëŠ” time indexë¥¼ ì‚¬ìš©í•œë‹¤. Hard Negative ë¬¸ì œëŠ” í•´ê²°í–ˆìœ¼ë‚˜ ì—¬ì „íˆ Hard Positive(ì¦ê°•ëœ ë‹¤ë¥¸ viewë¥¼ ë™ì¼í•˜ê²Œ ì·¨ê¸‰í•˜ëŠ” ë¬¸ì œ)ëŠ” ë‚¨ì•„ìˆë‹¤.

**Tags:** `#timeseries` `#contrastive-learning` `#model-agnostic` `#representation-learning`

</details>

<details>
<summary><b>(AutoTCL) PARAMETRIC AUGMENTATION FOR TIME SERIES CONTRASTIVE LEARNING</b></summary>

**Links:** [PDF](papers/Time%20Series/Representation%20Learning/(AutoTCL)%20PARAMETRIC%20AUGMENTATION%20FOR%20TIME%20SERIES%20CONTRASTIVE%20LEARNING.pdf) | [Notes](notes/Time%20Series/Representation%20Learning/(AutoTCL)%20PARAMETRIC%20AUGMENTATION%20FOR%20TIME%20SERIES%20CONTRASTIVE%20LEARNING.md)

**ì„¤ëª…:** ê¸°ì¡´ì˜ ê³ ì •ëœ augmentation ê¸°ë²•(Jittering, Scaling)ì´ ì‹œê³„ì—´ì˜ ë³¸ì§ˆì  íŠ¹ì„±ì„ ë°˜ì˜í•˜ì§€ ëª»í•œë‹¤ê³  ì§€ì í•˜ë©°, í•™ìŠµ ê°€ëŠ¥í•œ Augmentation Networkë¥¼ ì œì•ˆ. ì´ ë„¤íŠ¸ì›Œí¬ëŠ” ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì„ íƒí•˜ëŠ” ë§ˆìŠ¤í¬(h)ì™€ scaling ë³€í™˜ í•¨ìˆ˜(g)ë¥¼ ìƒì„±í•˜ë©°, Latent Spaceì—ì„œ dropout ë°©ì‹ìœ¼ë¡œ augmentationì„ ìˆ˜í–‰í•œë‹¤. ì •ë³´ ì´ë¡  ê´€ì ì—ì„œ ì—”íŠ¸ë¡œí”¼ê°€ ë†’ì€ ë·°ë¥¼ ìƒì„±í•˜ì—¬ ì •ë³´ ì´ë“ì„ ê·¹ëŒ€í™”í•œë‹¤. ë‹¤ë§Œ, ë§ì€ lossì™€ hyper-parameterë¡œ ì¸í•´ ì‚¬ìš©ì´ ë³µì¡í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.

**Tags:** `#timeseries` `#representation-learning` `#contrastive-learning` `#model-agnostic`

</details>

---

**ì´ ë…¼ë¬¸ ìˆ˜:** 8í¸ (ì½ì€ ë…¼ë¬¸: 6í¸, ì½ì§€ ì•Šì€ ë…¼ë¬¸: 2í¸)
