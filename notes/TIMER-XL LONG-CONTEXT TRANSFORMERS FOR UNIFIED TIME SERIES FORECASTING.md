> [!PDF|] [[TIMER-XL LONG-CONTEXT TRANSFORMERS FOR UNIFIED TIME SERIES FORECASTING.pdf#page=2&selection=136,29,142,1|TIMER-XL LONG-CONTEXT TRANSFORMERS FOR UNIFIED TIME SERIES FORECASTING, p.2]]
> > he training objective of language modeling to multivariate next token prediction, achieving unified time series forecasting that covers tasks in Figure 1 (right). Based on the decoder-only architecture, we propose
> 
> 